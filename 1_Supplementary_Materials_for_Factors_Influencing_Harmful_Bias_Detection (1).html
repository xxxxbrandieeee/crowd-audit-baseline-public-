<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection (1)</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 17pt; }
 .h2, h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; margin:0pt; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s8 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s10 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s11 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s14 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s15 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .a { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s16 { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s17 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c2, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 li {display: block; }
 #l3 {padding-left: 0pt;counter-reset: d1 1; }
 #l3> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><h1 style="padding-top: 5pt;text-indent: 0pt;text-align: center;">Supplementary Materials</h1><ol id="l1"><li data-list-text="1."><h2 style="padding-top: 19pt;padding-left: 25pt;text-indent: -19pt;text-align: left;">Distribution of responses for Bias Case Sets</h2><p style="padding-top: 9pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><span><img width="553" height="348" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_001.jpg"/></span></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Fig. 1: Distribution of responses for all three Bias Case Sets: For Case 1, Case 2, and Case 3, above, there are three separate graphs shown side-by-side for each case; the first graph for each case shows the scores given by respondents who belonged to marginalized vs. non-marginalized gender demographics; in the second graph, marginalized vs. non-marginalized sexual orientation demographics; and in the third graph, marginalized vs. non-marginalized race demographics. On the X-axis, the three graphs for each case show the number of survey respondents who gave the image search results for the given bias case a Harmfulness Rating score of 1 - 7, where 1 = ‘Totally Unharmful’ and 7 = ‘Totally Harmful.’ The Y-axis is bias the ‘harmfulness’ rating level the survey respondents gave the image search results for the given case, and ranges from 1 = ‘Totally unharmful to 7 = ‘Totally harmful. In the Case 1 graphs, in purple, each graph shows the scores given by survey respondents belonging to marginalized demographics; in gray, each graph shows the scores given by survey respondents who do not belong to marginalized demographics. In the Case 2 graphs, in teal, each graph shows the scores given by survey respondents belonging to marginalized demographics; in gray, each graph shows the scores given by survey respondents who do not belong to marginalized demographics. In the Case 3 graphs, in green, each graph shows the scores given by survey respondents belonging to marginalized demographics; in gray, each graph shows the scores given by survey respondents who do not belong to marginalized demographics. Each case shows three charts, comparing ratings of non-marginalized vs marginalized groups with respect to gender, sexual orientation, and race.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2."><h2 style="padding-left: 25pt;text-indent: -19pt;text-align: left;">Survey Questions</h2><p class="s1" style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: left;">The following questions are for one of the experiment cases. We iterated all the four case sets in a randomized order and randomly selected one case out of each case category to each participant.</p><ol id="l2"><li data-list-text="A."><p class="s2" style="padding-left: 18pt;text-indent: -12pt;text-align: left;">Questions about Detection of Harmful Bias and Discrimination in the case</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">These are the search results for “[INSERT SEARCH QUERY]” from an image search system. It was</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">captured from a [INSERT INTERFACE (COMPUTER / PHONE)] screen. Please look at the results carefully and answer all the questions below.</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">[INSERT IMAGE]</p><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q1: <span class="p">Please select the statement that indicates how much or how little harmful or unharmful bias and discrimination you think the algorithmic system is generating with this image. There are no right or wrong answers. (Choices: “Totally harmful”, “Very harmful”, “Somewhat harmful”, “Neither harmful nor unharmful”, “Somewhat unharmful”, “Very unharmful”, “Totally unharmful”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q2.1: <span class="p">Why do you believe this algorithmic system is generating [INSERT ANSWER FROM Q1] bias and discrimination? There are no right or wrong answers. Please explain your choice in a sentence starting with “I ... because . . . ”. [Text entry]</span></h2><p style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">(If the answer to Q1 is “Totally harmful”, “Very harmful”, “Somewhat harmful”, or “Neither harmful nor unharmful”)</p><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q2.2: <span class="p">What kinds of harmful bias and discrimination do you perceive and Who do you think might be harmed in this case? [Text entry]</span></h2></li><li data-list-text="B."><p class="s2" style="padding-left: 18pt;text-indent: -12pt;text-align: justify;">Questions about Everyday Discrimination Experience</p><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q9.1: <span class="p">In your day-to-day life how often have any of the following things happened to you? (“You are treated with less courtesy or respect than other people”: “Almost everyday”, “At least once a week”, “A few times a month”, “A few times a year”, “Less than once a year”, “Never”; “You receive poorer service than other people at restaurants or stores”: “Almost everyday”, “At least once a week”, “A few times a month”, “A few times a year”, “Less than once a year”, “Never”; “People act as if they think you are not smart”: “Almost everyday”, “At least once a week”, “A few times a month”, “A few times a year”, “Less than once a year”, “Never”; “People act as if they are afraid of you”: “Almost everyday”, “At least once a week”, “A few times a month”, “A few times a year”, “Less than once a year”, “Never”; “You are threatened or harassed”: “Almost everyday”, “At least once a week”, “A few times a month”, “A few times a year”, “Less than once a year”, “Never”)</span></h2><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">(If the answer to Q9.1 is “A few times a year” or more frequently to at least one proxy.)</p><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q9.2: <span class="p">What do you think is the main reason for these experiences? (Select all that apply) (Choices: “Your Ancestry or National Origins”, “Your Gender”, “Your Race”, “Your Age”, “Your Disability”, “Your Religion”, “Your Height”, “Your Weight”, “Some other Aspect of Your Physical Appearance”, “Your Sexual Orientation”, “Your Education”, “Income Level”, Other: [text entry]”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q10: <span class="p">Please select from the following that which most accurately represents the answer to the following question: Mary has 5 apples. If Josh gives her 3 oranges and 4 apples, how many apples will she have? (Choices: “7”, “9”, “8”, “6”)</span></h2></li><li data-list-text="C."><p class="s2" style="padding-left: 18pt;text-indent: -12pt;text-align: justify;">Questions about Socio-Technical Knowledge</p><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q11: <span class="p">Do you have close friends / family members who are members of racial minority (including all non-white ethnicity) groups? (Choices: “Yes”, “No”, “I don’t know”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q12: <span class="p">Do you have close friends / family members who are members of gender identity minority (in- cluding all non-male) groups? (Choices: “Yes”, “No”, “I don’t know”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q13: <span class="p">Do you have close friends / family members who are members of sexual orientation minority (including all non-heterosexual) groups? (Choices: “Yes”, “No”, “I don’t know”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q14: <span class="p">How frequently would you say you encounter news/media about bias in society? (Choices: “Daily”, “Weekly”, “Monthly”, “A few times a year”, “Never”, “I don’t know”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q15: <span class="p">How frequently would you say you encounter news/media about bias in algorithms or automated systems used by people or organizations? (Choices: “Daily”, “Weekly”, “Monthly”, “A few times a year”, “Never”, “I don’t know”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q16: <span class="p">How familiar are you with the use of algorithmic systems (e.g., email spam filter, social media, amazon recommendations, etc)? (Choices: “Extremely familiar”, “Moderately familiar”, “Neither famil- iar nor not familiar”, “Moderately not familiar”, “Not familiar at all”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q17: <span class="p">How aware do you feel you are with issues related to societal biases (e.g., racial biases, gender biases)? (Choices: “Very aware ”, “Somewhat aware”, “Neither aware nor not aware”, “Not very aware”, “Not at all aware”)</span></h2></li><li data-list-text="D."><p class="s2" style="padding-top: 3pt;padding-left: 19pt;text-indent: -13pt;text-align: justify;">Questions about Demographics</p><h2 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Q18: <span class="p">What is your age? (Choices: “18-24”, “25-34”, “35-44”, “45-54”, “55-64”, “65 or above”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q19: <span class="p">What is your Race/Ethnicity? Please select all that apply. (Choices: “White”, “Black or African American”, “American Indian or Alaska Native”, “Asian”, “Native Hawaiian or Other Pacific Islander”, “Hispanic”, “Two or More Races”, “Other Race [Text Entry]”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: left;">Q20: <span class="p">What is your Gender Identity? (Choices: “Male”, “Female”, “Trans Male/Trans Man”, “Trans Fe- male/Trans Woman”, “Genderqueer/Gender Non Conforming”, “Different Identity”, “Rather not say”) </span>Q21: <span class="p">Does your current gender differ from the one you were assigned at birth? (Choices: “Yes”, “No”, “Rather not say”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: left;">Q22: <span class="p">What is your Sexual Orientation? (Choices: “Heterosexual”, “Homosexual”, “Bisexual”, “Asex- ual”, “Other [Text Entry]”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Q23: <span class="p">What is the highest degree or level of school you have completed (if you’re currently enrolled in school, please indicate the highest degree you have received). (Choices: “Less than a high school diploma”, “High School degree or equivalent”, “Some college, no degree”, “Associate degree”, “Bach- elor’s degree”, “Master’s degree”, “Professional degree”, “Doctorate”)</span></h2><h2 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Q24: <span class="p">What is the zip code of your current residence? [Text Entry]</span></h2></li><li data-list-text="E."><p class="s2" style="padding-left: 18pt;text-indent: -12pt;text-align: justify;">Debriefing Session</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">We previously informed you the purpose of the study is asking for your opinion about algorithmic systems. The goal of our research is to find out factors influencing users’ bias detection in algorithm systems. The following case (we showed you previously in this survey) has been identified as having harmful algorithmic bias and discrimination against [ADDRESS THE IMPACTED DEMOGRAPH- ICS].</p><h2 style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: left;">Q25: <span class="p">Before taking this survey, have you seen or heard about this image search result and its harmful impacts before? [ITERATIVELY INSERT THE IMAGE PREVIOUSLY SHOWN IN THE SURVEY]</span></h2><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">(Choices: “Yes”, “No”, “I’m not sure”)</p></li></ol></li><li data-list-text="3."><h2 style="padding-top: 3pt;padding-left: 25pt;text-indent: -19pt;text-align: left;">Participant Demographics</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: center;">Table 1: Demographics statistics of our survey sample (<i>N </i><span class="s3">= </span>2<span class="s4">, </span>179).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:117.451pt" cellspacing="0"><tr style="height:14pt"><td style="width:147pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s5" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Demographic Characteristics</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s6" style="padding-top: 1pt;padding-right: 8pt;text-indent: 0pt;text-align: right;">N</p></td><td style="width:35pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s5" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">%</p></td></tr><tr style="height:22pt"><td style="width:147pt;border-top-style:solid;border-top-width:1pt"><p class="s7" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Gender</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Female</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt"><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">969</p></td><td style="width:35pt;border-top-style:solid;border-top-width:1pt"><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">44<span class="s8">.</span>47%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Male</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1132</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">51<span class="s8">.</span>95%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Genderqueer</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">41</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1<span class="s8">.</span>88%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Prefer not to disclose</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">11</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">0<span class="s8">.</span>50%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Transgender</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">21</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">0<span class="s8">.</span>96%</p></td></tr><tr style="height:10pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Different Identity</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">5</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">0<span class="s8">.</span>23%</p></td></tr><tr style="height:21pt"><td style="width:147pt"><p class="s7" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Sexual Orientation</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Heterosexual</p></td><td style="width:42pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1677</p></td><td style="width:35pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">76<span class="s8">.</span>96%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-right: 19pt;text-indent: 0pt;line-height: 9pt;text-align: right;">Gay, Lesbian, Bisexual, or Asexual</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">454</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">20<span class="s8">.</span>84%</p></td></tr><tr style="height:10pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Other</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">48</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">2<span class="s8">.</span>20%</p></td></tr><tr style="height:21pt"><td style="width:147pt"><p class="s7" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Race</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">American Indian or Alaska Native</p></td><td style="width:42pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">6</p></td><td style="width:35pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">0<span class="s8">.</span>28%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Asian</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">226</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">10<span class="s8">.</span>37%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Black or African American</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">283</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">12<span class="s8">.</span>99%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Hispanic</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">142</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">6<span class="s8">.</span>52%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-right: 18pt;text-indent: 0pt;line-height: 9pt;text-align: right;">Native Hawaiian or Pacific Islander</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">3</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">0<span class="s8">.</span>14%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">White</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1314</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">60<span class="s8">.</span>30%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Other Race</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">10</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">0<span class="s8">.</span>46%</p></td></tr><tr style="height:10pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Two or More Races</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">195</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">8<span class="s8">.</span>94%</p></td></tr><tr style="height:21pt"><td style="width:147pt"><p class="s7" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Age</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">18–24</p></td><td style="width:42pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">309</p></td><td style="width:35pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">14<span class="s8">.</span>18%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">25–34</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">702</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">32<span class="s8">.</span>22%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">35–44</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">524</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">24<span class="s8">.</span>05%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">45–54</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">313</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">14<span class="s8">.</span>36%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">55–64</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">206</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">9<span class="s8">.</span>45%</p></td></tr><tr style="height:10pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">65+</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">125</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">5<span class="s8">.</span>74%</p></td></tr><tr style="height:21pt"><td style="width:147pt"><p class="s7" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Education</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Less than a high school diploma</p></td><td style="width:42pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">23</p></td><td style="width:35pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1<span class="s8">.</span>06%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">High school degree or equivalent</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">275</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">12<span class="s8">.</span>62%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Some college, no degree</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">495</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">22<span class="s8">.</span>72%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Associate Degree</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">222</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">10<span class="s8">.</span>19%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Bachelor’s degree</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">821</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">37<span class="s8">.</span>68%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Master’s degree</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">258</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">11<span class="s8">.</span>84%</p></td></tr><tr style="height:11pt"><td style="width:147pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Professional degree</p></td><td style="width:42pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">43</p></td><td style="width:35pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1<span class="s8">.</span>97%</p></td></tr><tr style="height:12pt"><td style="width:147pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s5" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Doctorate</p></td><td style="width:42pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s5" style="padding-right: 7pt;text-indent: 0pt;text-align: right;">42</p></td><td style="width:35pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s5" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">1<span class="s8">.</span>93%</p></td></tr></table><p class="s9" style="padding-top: 2pt;padding-left: 121pt;text-indent: 0pt;line-height: 110%;text-align: left;">1 <span class="s10">For gender, our source data from U.S. Census only have female and male percentage.</span></p><p class="s9" style="padding-left: 121pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2 <span class="s10">For Race, our source data from U.S. Census doesn’t have Middle</span></p><p class="s11" style="padding-left: 121pt;text-indent: 0pt;text-align: left;">Eastern as a separate race.</p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="4."><h2 style="padding-left: 25pt;text-indent: -19pt;text-align: left;">Diagram of the Overview of the Study Procedure</h2></li><li data-list-text="5."><h2 style="padding-top: 7pt;padding-left: 25pt;text-indent: -19pt;text-align: left;">Filtering Low-Quality Responses using LLM</h2><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 107%;text-align: justify;">When we further inspected our data, we found that a number of participants’ responses asking for their rationale behind their ratings (Q2) were contradictory to their ratings (Q1). For example, one participant responded <i>“I believe this is harmful” </i>while rating the case as <i>“Very unharmful”</i><a href="#bookmark0" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. To filter out likely noise in our data, we used a Large Language Model (LLM)</a><span style=" color: #F00; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">1</span> to label participants’ responses in the open-ended questions, following best practices in prior work [<b>?</b>]. All responses underwent a de-identification process in compliance with our IRB. To validate the use of LLM, we sampled 100 participants from the original 2,201, yielding 400 responses to Q2. Two human coders labeled these as <i>”Yes” </i>(the participant felt the case contains harmful bias) or <i>”No”</i>. After achieving an intercoder reliability of 0.920, exceeding the recommended 0.7 threshold [<b>?</b>], we used the 400 hand-coded responses as ground truth, and divided this</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="241" height="1" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_002.png"/></span></p><p class="s14" style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">&zwnj;</a>1<span class="s15">https://openai.com/product/gpt-4</span></p><p style="padding-left: 12pt;text-indent: 0pt;text-align: left;"><span><img width="559" height="178" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_003.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Fig. 2: <b>Overview of the study procedure. </b>In each survey, participants were asked to complete 5 steps:</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">(1) Rate four different cases of image search results in random order, in terms of how harmful the bias and discrimination is and offer rationale in open-ended questions; (2) Answer questions about their ev- eryday discrimination experiences; (3) Answer questions regarding their social and technical knowledge;</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">(4) Answer demographic questions; (5) Complete the debrief session.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 107%;text-align: justify;">set into a validation set (200 instances) and a test set (200 instances). After iterative prompt engineering, we settled on a final prompt that had the highest accuracy level of 98.7% and a kappa value of 0.974 on the validation set, which is: <i>“For the given statement about an algorithmic system, identify whether it indicates the user feels the system contains harmful biases. Answer the question with “Yes”, “No”, “Uncertain”.” </i>We then used the LLM to analyze the remaining 8,804 responses from all 2,201 partic- ipants. A human coder reviewed and hand coded all responses labeled as <i>“Uncertain”</i>. We identified 22 participants that had inconsistencies between their answers to Q1 and Q2. Our final data set contains 2,179 participants after excluding 22 participants.</p></li><li data-list-text="6."><h2 style="padding-top: 12pt;padding-left: 25pt;text-indent: -19pt;text-align: justify;">Cases We Use in the Survey Experiment</h2></li></ol><p style="padding-top: 8pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 64pt;text-indent: 0pt;text-align: left;"><span><img width="442" height="241" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_004.jpg"/></span></p><p class="s16" style="padding-top: 11pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span style=" color: #000;">Fig. 3: </span><span class="h2">Bias Case Set 1 - 1: </span><a href="#bookmark4" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The image search results of “professor style” on Google Images were identified as having gender bias because of only displaying images of male [</a>4<a href="#bookmark4" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">]. We used the screenshot of the image search results presented in the literature, which was captured from a computer screen [</a>4<span style=" color: #000;">], and cropped out all the text in the screenshot to use in the survey experiment.</span></p><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><span><img width="450" height="137" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_005.jpg"/></span></p><p class="s16" style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span style=" color: #000;">Fig. 4: </span><span class="h2">Bias Case Set 1 - 2: </span><a href="#bookmark4" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The image search results of “doctor” on Google Images were identified as having gender bias because of only showing white male doctors [</a>4<a href="#bookmark4" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">]. We used the screenshot of the image search results presented in the literature, which was captured from a computer screen [</a>4<span style=" color: #000;">], and cropped out all the text in the screenshot to use in the survey experiment.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 65pt;text-indent: 0pt;text-align: left;"><span><img width="446" height="175" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_006.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 5pt;text-indent: 0pt;line-height: 107%;text-align: left;"><span style=" color: #000;">Fig. 5: </span><span class="h2">Bias Case Set 2 - 1: </span><a href="#bookmark2" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The image search results of “weddings” on Google Images were identified as having sexual orientation bias because of only portraying heterosexual couples [</a>2<a href="https://www.google.com/search?sca_esv=562916950&amp;sxsrf=AB5stBiKKLCxOFQn5F4dw4Kz0KzS07bWCg%3A1693964114308&amp;q=weddings&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ved=2ahUKEwjfja6K7JSBAxXsGFkFHTO7BCkQ0pQJegQIDRAB&amp;biw=1728&amp;bih=994&amp;dpr=2" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">]. One researcher recaptured a screenshot of the first page of Google image search results for “wed- dings” from a computer interface and cropped out all text, </a><a href="https://www.google.com/search?sca_esv=562916950&amp;sxsrf=AB5stBiKKLCxOFQn5F4dw4Kz0KzS07bWCg%3A1693964114308&amp;q=weddings&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ved=2ahUKEwjfja6K7JSBAxXsGFkFHTO7BCkQ0pQJegQIDRAB&amp;biw=1728&amp;bih=994&amp;dpr=2" class="a" target="_blank">https://www.google.com/ search?sca_esv=562916950&amp;sxsrf=AB5stBiKKLCxOFQn5F4dw4Kz0KzS07bWCg: 1693964114308&amp;q=weddings&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ved= </a>2ahUKEwjfja6K7JSBAxXsGFkFHTO7BCkQ0pQJegQIDRAB&amp;biw=1728&amp;bih=994&amp;dpr=2<span style=" color: #000;">, Accessed: 2022-10-22.</span></p><p style="padding-left: 145pt;text-indent: 0pt;text-align: left;"><span><img width="230" height="347" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_007.jpg"/></span></p><p class="s16" style="padding-top: 12pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span style=" color: #000;">Fig. 6: </span><span class="h2">Bias Case Set 2 - 2: </span><a href="#bookmark2" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The image search results of “romantic couples” on Google Images were identified as having sexual orientation bias because of only portraying heterosexual couples [</a>2<a href="#bookmark2" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">]. We used the screenshot of the image search results presented in the literature, which was captured from a phone screen [</a>2<span style=" color: #000;">], and cropped out all the text in the screenshot to use in the survey experiment.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><span><img width="449" height="200" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_008.jpg"/></span></p><p class="s16" style="padding-top: 11pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span style=" color: #000;">Fig. 7: </span><span class="h2">Bias Case Set 3 - 1: </span><a href="#bookmark3" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The image search results of “babies” by Microsoft search engine Bing were identified as having racial bias because of displaying white babies [</a>3<a href="#bookmark3" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">]. We used the screenshot of the image search results presented in the literature, which was captured from a phone screen [</a>3<span style=" color: #000;">], and cropped out all the text in the screenshot to use in the survey experiment.</span></p><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><span><img width="449" height="132" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_009.jpg"/></span></p><p class="s16" style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span style=" color: #000;">Fig. 8: </span><span class="h2">Bias Case Set 3 - 2: </span><a href="#bookmark1" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The image search results of “CEO” on Google Images were identified as having racial bias because of only showing images of white male [</a>1<a href="#bookmark1" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">]. We used the screenshot of the image search results presented in the literature, which was captured from a phone screen [</a>1<span style=" color: #000;">], and cropped out all the text in the screenshot to use in the survey experiment.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 65pt;text-indent: 0pt;text-align: left;"><span><img width="440" height="164" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_010.jpg"/></span></p><p style="padding-top: 12pt;padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Fig. 9: <b>Neutral Case Set - 1 </b><a href="https://www.google.com/search?q=flower&amp;sca_esv=562916950&amp;tbm=isch&amp;source=hp&amp;biw=1571&amp;bih=874" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">The image search results of “flower” on Google Images were consid- ered as a neutral case because of not involving human subject. One researcher captured a screenshot of the first page of Google image search results for“flowe” from a computer interface and cropped out all text, </a><a href="https://www.google.com/search?q=flower&amp;sca_esv=562916950&amp;tbm=isch&amp;source=hp&amp;biw=1571&amp;bih=874" class="a" target="_blank">https://www.google.com/search?q=flower&amp;sca_esv=562916950&amp;tbm= </a><span style=" color: #00F;">isch&amp;source=hp&amp;biw=1571&amp;bih=874</span>, Accessed: 2022-10-19.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 64pt;text-indent: 0pt;text-align: left;"><span><img width="441" height="172" alt="image" src="1_Supplementary_Materials_for_Factors_Influencing_Harmful_Bias_Detection%20%281%29_files/Image_011.jpg"/></span></p><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Fig. 10: <b>Neutral Case Set - 2 </b><a href="https://www.google.com/search?q=tree&amp;sca_esv=562916950&amp;tbm=isch&amp;source=hp&amp;biw=1571&amp;bih=874" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">The image search results of “tree” on Google Images were consid- ered as a neutral case because of not involving human subject. One researcher captured a screen- shot of the first page of Google image search results for“flowe” from a computer interface and cropped out all text, </a><a href="https://www.google.com/search?q=tree&amp;sca_esv=562916950&amp;tbm=isch&amp;source=hp&amp;biw=1571&amp;bih=874" class="a" target="_blank">https://www.google.com/search?q=tree&amp;sca_esv=562916950&amp; </a><span style=" color: #00F;">tbm=isch&amp;source=hp&amp;biw=1571&amp;bih=874</span>, Accessed: 2022-10-19.</p><p style="padding-top: 10pt;text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">References</h2><ol id="l3"><li data-list-text="1."><p class="s17" style="padding-top: 6pt;padding-left: 33pt;text-indent: -12pt;text-align: left;"><a name="bookmark1">&zwnj;</a>BREKKE, K. Google image search has a gender bias problem. <i>1</i>, 2015. Accessed: 2023-09-05.</p></li><li data-list-text="2."><p class="s17" style="padding-top: 3pt;padding-left: 33pt;text-indent: -12pt;text-align: left;"><a name="bookmark2">&zwnj;</a>DEVOS, A., DHABALIA, A., SHEN, H., HOLSTEIN, K., AND ESLAMI, M. Toward user-driven algo-</p><p class="s17" style="padding-left: 33pt;text-indent: 0pt;text-align: left;"><a name="bookmark3">&zwnj;</a>rithm auditing: Investigating users’ strategies for uncovering harmful algorithmic behavior. In <i>CHI Confer- ence on Human Factors in Computing Systems </i>(2022).</p></li><li data-list-text="3."><p class="s17" style="padding-left: 33pt;text-indent: -12pt;text-align: left;"><a name="bookmark4">&zwnj;</a>KLEINMAN, Z. Artificial intelligence: How to avoid racist algorithms. 2017.</p></li><li data-list-text="4."><p class="s17" style="padding-top: 1pt;padding-left: 33pt;text-indent: -12pt;text-align: left;">NOBLE, S. U. Algorithms of oppression: How search engines reinforce racism. NYU Press, 2018.</p></li></ol></body></html>
